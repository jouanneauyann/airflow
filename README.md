# ğŸ—ï¸ Data Platform â€“ Ingestion - Transformation - Restitution

Ce projet met en Å“uvre une **data plateforme** basÃ©e sur Airflow, Python, Snowflake, DBT et QlikSense.  
Il permet lâ€™ingestion automatisÃ©e de donnÃ©es depuis plusieurs sources (APIs, bases de donnÃ©es, fichiers plats), leur transformation avec DBT, puis leur exposition Ã  notre outil de visualisation : QlikSense.

---

## ğŸ  Architecture Globale

        [Sources externes : API, SQL, SharePoint]
                        â”‚
                        â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Airflow + Python DAGs
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
      [Fichiers temporaires crÃ©Ã©s sur le serveur airflow]
                        â”‚
                        â–¼
            [Ingestion dans Snowflake ]
                        â”‚
                        â–¼
             [Transformation avec DBT]
                        â”‚
                        â–¼
            [Visualisation via QlikSense]

## ğŸ“‚ Organisation du rÃ©pertoire Globale
```
â”œâ”€â”€ config/ # Fichiers de configuration permettant de gÃ©rer la connexion Ã  Snowflake et les paramÃ¨tres par dÃ©faut d'un DAG
â”‚ â”œâ”€â”€ airflow.cfg
â”‚ â”œâ”€â”€ config_snowflake.json
â”‚ â”œâ”€â”€ default_args.py
â”‚ â”œâ”€â”€ rsa_key.pem
â”‚ â””â”€â”€ rsa_key.pub
â”‚ 
â”œâ”€â”€ dags/ # Fichiers DAG Airflow (un par source ou type d'ingestion)
â”‚ â”œâ”€â”€ ingestion_<source>.py # dags d'ingestion d'une source vers Snowflake
â”‚ â”œâ”€â”€ actualisation_liste_sharepoint.py # dag divers permettant de mettre Ã  jour des listes dans SharePoint
â”‚ â””â”€â”€ log_maintenance.py 
â”‚
â”œâ”€â”€ scripts/ # Scripts Python dâ€™ingestion et logique mÃ©tier
â”‚ â””â”€â”€ ingestion/
â”‚   â”œâ”€â”€ ingestion_api.py
â”‚   â”œâ”€â”€ ingestion_sql.py
â”‚   â”œâ”€â”€ ingestion_sharepoint.py
â”‚   â”œâ”€â”€ module_global.py
â”‚ â””â”€â”€ <source>/
â”‚   â””â”€â”€ config_<source>.yaml # Fichier YAML propre Ã  chaque source
â”‚
â”œâ”€â”€ data/ # DonnÃ©es temporaires extraites avant ingestion dans Snowflake
â”‚ â””â”€â”€ <source>/<requete>/<fichiers>.<extension> (uniquement CSV, JSON ou PARQUET)
â”‚
â”œâ”€â”€ logs/ # Logs dâ€™exÃ©cution Airflow et Python
â”‚
â”œâ”€â”€ zabbix/ # Zabbix monitore le bon fonctionnement des dags de faÃ§on Ã  remonter une alerte en cas de fichier prÃ©sent dans ce dossier
â”‚
â””â”€â”€ README.md # PrÃ©sentation du projet (ce fichier)
```

---

## âš™ï¸ Fonctionnement dâ€™un DAG d'ingestion

Chaque DAG d'ingestion suit la logique suivante :

1. **Connexion Ã  la source** (API, BDD, SharePoint)
2. **ExÃ©cution des requÃªtes** (configurÃ©es dans un fichier `yaml`)
3. **Ã‰criture locale des rÃ©sultats** (CSV/JSON/PARQUET dans `/data`)
4. **Chargement dans Snowflake** (Import des requÃªtes dans un stage)

Les scripts Python sont **gÃ©nÃ©riques** et contrÃ´lÃ©s par des **fichiers YAML** propres Ã  chaque source. Ainsi, pour chaque type de source, il y a un script python qui appel le fichier de configuration YAML associÃ©e Ã  la source.

---

## ğŸ› ï¸ Technologies utilisÃ©es

1. **Airflow** : Orchestration des pipelines
2. **Python** : Scripts dâ€™extraction & ingestion
3. **Snowflake** : Stockage & compute des donnÃ©es
4. **DBT** : Transformation & modÃ©lisation des donnÃ©es
5. **QlikSense** : Visualisation et reporting